{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fb5af0",
   "metadata": {},
   "source": [
    "# Augmented Test to SQL Grammar Parser\n",
    "Uses an augmented context free grammar (CFG) to parse natural language queries into SQL queries to search the Air Traffic Information Systems (ATIS) database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ccc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from cryptography.fernet import Fernet\n",
    "import copy\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import wget\n",
    "import sqlite3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.legacy as tt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from cryptography.fernet import Fernet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f5c24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "# Set timeout for executing SQL\n",
    "TIMEOUT = 3 # seconds\n",
    "\n",
    "# GPU check: Set runtime type to use GPU where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db471636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 16404480 / 16404480"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data//atis_sqlite (1).db'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download needed scripts and data\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "source_url = \"https://raw.githubusercontent.com/nlp-course/data/master\"\n",
    "\n",
    "# Grammar to augment\n",
    "if not os.path.isfile('data/grammar'):\n",
    "  wget.download(f\"{source_url}/ATIS/grammar_distrib4.crypt\", out=\"data/\")\n",
    "\n",
    "  # Decrypt the grammar file\n",
    "  key = b'bfksTY2BJ5VKKK9xZb1PDDLaGkdu7KCDFYfVePSEfGY='\n",
    "  fernet = Fernet(key)\n",
    "  with open('./data/grammar_distrib4.crypt', 'rb') as f:\n",
    "    restored = Fernet(key).decrypt(f.read())\n",
    "  with open('./data/grammar', 'wb') as f:\n",
    "    f.write(restored)\n",
    "\n",
    "# Download scripts and ATIS database\n",
    "wget.download(f\"{source_url}/scripts/trees/transform.py\", out=\"scripts/\")\n",
    "wget.download(f\"{source_url}/ATIS/atis_sqlite.db\", out=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d42237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import downloaded scripts for parsing augmented grammars\n",
    "sys.path.insert(1, './scripts')\n",
    "import transform as xform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bb90f",
   "metadata": {},
   "source": [
    "Load and preprocess the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3463a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 2591248 / 2591248"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data//train_flightid (1).sql'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acquire the datasets - training, development, and test splits of the \n",
    "# ATIS queries and corresponding SQL queries\n",
    "wget.download(f\"{source_url}/ATIS/test_flightid.nl\", out=\"data/\")\n",
    "wget.download(f\"{source_url}/ATIS/test_flightid.sql\", out=\"data/\")\n",
    "wget.download(f\"{source_url}/ATIS/dev_flightid.nl\", out=\"data/\")\n",
    "wget.download(f\"{source_url}/ATIS/dev_flightid.sql\", out=\"data/\")\n",
    "wget.download(f\"{source_url}/ATIS/train_flightid.nl\", out=\"data/\")\n",
    "wget.download(f\"{source_url}/ATIS/train_flightid.sql\", out=\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538b47",
   "metadata": {},
   "source": [
    "Use torchtext to process the data, with field SRC for the natural language questions and TGT for the SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca3542bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\d+|st\\.|[\\w-]+|\\$[\\d\\.]+|\\S+')\n",
    "def tokenize(string):\n",
    "  return tokenizer.tokenize(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66f6204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = tt.data.Field(include_lengths=True,         # include lengths\n",
    "                    batch_first=False,            # batches will be max_len x batch_size\n",
    "                    tokenize=tokenize,            # use our tokenizer\n",
    "                   ) \n",
    "TGT = tt.data.Field(include_lengths=False,\n",
    "                    batch_first=False,            # batches will be max_len x batch_size\n",
    "                    tokenize=lambda x: x.split(), # use split to tokenize\n",
    "                    init_token=\"<bos>\",           # prepend <bos>\n",
    "                    eos_token=\"<eos>\")            # append <eos>\n",
    "fields = [('src', SRC), ('tgt', TGT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b5fd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocab: 421\n",
      "Most common English words: [('to', 3478), ('from', 3019), ('flights', 2094), ('the', 1550), ('on', 1230), ('me', 973), ('flight', 972), ('show', 845), ('what', 833), ('boston', 813)]\n",
      "\n",
      "Size of SQL vocab: 392\n",
      "Most common SQL words: [('=', 38876), ('AND', 36564), (',', 22772), ('airport_service', 8314), ('city', 8313), ('(', 6432), (')', 6432), ('flight_1.flight_id', 4536), ('flight', 4221), ('SELECT', 4178)]\n",
      "\n",
      "Index for start of sequence token: 2\n",
      "Index for end of sequence token: 3\n"
     ]
    }
   ],
   "source": [
    " # Make splits for data\n",
    "train_data, val_data, test_data = tt.datasets.TranslationDataset.splits(\n",
    "    ('_flightid.nl', '_flightid.sql'), fields, path='./data/',\n",
    "    train='train', validation='dev', test='test')\n",
    "\n",
    "MIN_FREQ = 3\n",
    "SRC.build_vocab(train_data.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train_data.tgt, min_freq=MIN_FREQ)\n",
    "\n",
    "print (f\"Size of English vocab: {len(SRC.vocab)}\")\n",
    "print (f\"Most common English words: {SRC.vocab.freqs.most_common(10)}\\n\")\n",
    "\n",
    "print (f\"Size of SQL vocab: {len(TGT.vocab)}\")\n",
    "print (f\"Most common SQL words: {TGT.vocab.freqs.most_common(10)}\\n\")\n",
    "\n",
    "print (f\"Index for start of sequence token: {TGT.vocab.stoi[TGT.init_token]}\")\n",
    "print (f\"Index for end of sequence token: {TGT.vocab.stoi[TGT.eos_token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23173189",
   "metadata": {},
   "source": [
    "Batch the data to facilitate processing on a GPU. sort_key function allows for sorting on length, to minimize the padding on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e79df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 # batch size for training/validation\n",
    "TEST_BATCH_SIZE = 1 # batch size for test, we use 1 to make beam search implementation easier\n",
    "\n",
    "train_iter, val_iter = tt.data.BucketIterator.splits((train_data, val_data),\n",
    "                                                     batch_size=BATCH_SIZE, \n",
    "                                                     device=device,\n",
    "                                                     repeat=False, \n",
    "                                                     sort_key=lambda x: len(x.src), \n",
    "                                                     sort_within_batch=True)\n",
    "test_iter = tt.data.BucketIterator(test_data, \n",
    "                                   batch_size=TEST_BATCH_SIZE, \n",
    "                                   device=device,\n",
    "                                   repeat=False, \n",
    "                                   sort=False, \n",
    "                                   train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44fa0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "train_batch_text, train_batch_text_lengths = batch.src\n",
    "train_batch_sql = batch.tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5af8b",
   "metadata": {},
   "source": [
    "Set up a SQL database to test the parses correctly return the right database entries using sqlite3 module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe10c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql):\n",
    "  conn = sqlite3.connect('data/atis_sqlite.db')  # establish the DB based on the downloaded data\n",
    "  c = conn.cursor()                              # build a \"cursor\"\n",
    "  c.execute(sql)\n",
    "  results = list(c.fetchall())\n",
    "  c.close()\n",
    "  conn.close()\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f5c3a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/img/encoderdecoder_attn_1layer.png\" alt=\"encoder-decoder-attn illustration\" />\n",
    "\n",
    "Implement the class `AttnEncoderDecoder` to convert natural language queries into SQL statements. The following methods are used to implement the Encoder-Decoder\n",
    "\n",
    "* **Model**\n",
    "\n",
    "    1. `__init__`: an initializer to create network modules.\n",
    "\n",
    "    2. `forward`: given source word ids of size `(max_src_len, batch_size)`, source lengths of size `(batch_size)` and decoder input target word ids `(max_tgt_len, batch_size)`, returns logits `(max_tgt_len, batch_size, V_tgt)`. Implements two functions: `forward_encoder` and `forward_decoder`.\n",
    "\n",
    "* **Optimization**\n",
    "\n",
    "    3. `train_all`: compute loss on training data, compute gradients, and update model parameters to minimize the loss.\n",
    "\n",
    "    4. `evaluate_ppl`: evaluate the current model's perplexity on a given dataset iterator, we use the perplexity value on the validation set to select the best model.\n",
    "\n",
    "* **Decoding**\n",
    "\n",
    "    5. `predict`: Generates the target sequence given a list of source tokens using beam search decoding. Assume batch size of 1 for simplicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e5baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement global attention function\n",
    "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
    "  \"\"\"\n",
    "  Performs the attention operation and returns the attention matrix\n",
    "  `batched_A` and the context matrix `batched_C` using queries \n",
    "  `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
    "  \n",
    "  Arguments:\n",
    "      batched_Q: (q_len, bsz, D)\n",
    "      batched_K: (k_len, bsz, D)\n",
    "      batched_V: (k_len, bsz, D)\n",
    "      mask: (bsz, q_len, k_len). An optional boolean mask *disallowing* \n",
    "            attentions where the mask value is *`False`*.\n",
    "  Returns:\n",
    "      batched_A: the normalized attention scores (bsz, q_len, k_len)\n",
    "      batched_C: a tensor of size (q_len, bsz, D).\n",
    "  \"\"\"\n",
    "  # Check sizes\n",
    "  D = batched_Q.size(-1)\n",
    "  bsz = batched_Q.size(1)\n",
    "  q_len = batched_Q.size(0)\n",
    "  k_len = batched_K.size(0)\n",
    "  assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
    "  assert batched_K.size(1) == bsz and batched_V.size(1) == bsz\n",
    "  assert batched_V.size(0) == k_len\n",
    "  if mask is not None:\n",
    "    assert mask.size() == torch.Size([bsz, q_len, k_len])\n",
    "  K = torch.transpose(torch.transpose(batched_K, 0, 2), 0, 1) # gives (bsz, D, k_len)\n",
    "  Q = torch.transpose(batched_Q, 0, 1) # gives (bsz, q_len, D)\n",
    "  QK = torch.bmm(Q, K)\n",
    "  if mask != None:\n",
    "    mask = mask == False # change to where mask does not exist\n",
    "    QK = QK.masked_fill(mask, -math.inf) # fill in where there is no mask with - inf\n",
    "    \n",
    "  batched_A = torch.softmax(QK, dim = -1) # gives (bsz, q_len, k_len), only do masked fill if masked not none\n",
    "  batched_C = torch.transpose(torch.bmm(batched_A, torch.transpose(batched_V, 0, 1)), 0, 1) # gives (q_len, bsz, D)\n",
    "  # Verify that things sum up to one properly.\n",
    "  assert torch.all(torch.isclose(batched_A.sum(-1), \n",
    "                                 torch.ones(bsz, q_len).to(device)))\n",
    "  return batched_A, batched_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f88535e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam():\n",
    "  \"\"\"\n",
    "  Helper class for storing a hypothesis, its score and its decoder hidden state.\n",
    "  \"\"\"\n",
    "  def __init__(self, decoder_state, tokens, score):\n",
    "    self.decoder_state = decoder_state\n",
    "    self.tokens = tokens\n",
    "    self.score = score\n",
    "        \n",
    "class BeamSearcher():\n",
    "  \"\"\"\n",
    "  Main class for beam search.\n",
    "  \"\"\"\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "    self.bos_id = model.bos_id\n",
    "    self.eos_id = model.eos_id\n",
    "    self.padding_id_src = model.padding_id_src\n",
    "    self.V = model.V_tgt\n",
    "\n",
    "\n",
    "  def beam_search(self, src, src_lengths, K, max_T):\n",
    "    \"\"\"\n",
    "    Performs beam search decoding.\n",
    "    Arguments:\n",
    "        src: src batch of size (max_src_len, 1)\n",
    "        src_lengths: src lengths of size (1)\n",
    "        K: beam size\n",
    "        max_T: max possible target length considered\n",
    "    Returns:\n",
    "        a list of token ids and a list of attentions\n",
    "    \"\"\"\n",
    "    finished = []\n",
    "    all_attns = []\n",
    "    # Initialize the beam\n",
    "    self.model.eval()\n",
    "    # find memory bank, encoder final state, and initialize beams\n",
    "\n",
    "    memory_bank, encoder_final_state = self.model.forward_encoder(src, src_lengths)\n",
    "    init_beam = Beam(encoder_final_state, [torch.tensor([self.bos_id])], 0)\n",
    "    beams = [init_beam]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for t in range(max_T): # main body of search over time steps\n",
    "        \n",
    "        # Expand each beam by all possible tokens y_{t+1}\n",
    "        all_total_scores = []\n",
    "        for beam in beams:\n",
    "          y_1_to_t, score, decoder_state = beam.tokens, beam.score, beam.decoder_state\n",
    "          y_t = y_1_to_t[-1]\n",
    "          #TODO - finish the code below\n",
    "          # Hint: you might want to use `model.forward_decoder_incrementally` with `normalize=True`\n",
    "          \n",
    "          src_mask = src.ne(self.padding_id_src)\n",
    "          logits, decoder_state, attn = \\\n",
    "          self.model.forward_decoder_incrementally(decoder_state, \n",
    "                                                   y_t, \n",
    "                                                   memory_bank, \n",
    "                                                   src_mask, \n",
    "                                                   normalize = True)\n",
    "          total_scores = logits + score\n",
    "          all_total_scores.append(total_scores)\n",
    "          all_attns.append(attn) # keep attentions for visualization\n",
    "          beam.decoder_state = decoder_state # update decoder state in the beam\n",
    "        all_total_scores = torch.stack(all_total_scores) # (K, V) when t>0, (1, V) when t=0\n",
    "\n",
    "        # Find K best next beams\n",
    "        # The code below has the same functionality as line 6-12, but is more efficient\n",
    "        all_scores_flattened = all_total_scores.view(-1) # K*V when t>0, 1*V when t=0\n",
    "        topk_scores, topk_ids = all_scores_flattened.topk(K, 0)\n",
    "        beam_ids = topk_ids.div(self.V, rounding_mode='floor')\n",
    "        next_tokens = topk_ids - beam_ids * self.V\n",
    "        new_beams = []\n",
    "        for k in range(K):\n",
    "          beam_id = beam_ids[k]       # which beam it comes from\n",
    "          y_t_plus_1 = next_tokens[k] # which y_{t+1}\n",
    "          score = topk_scores[k]\n",
    "          beam = beams[beam_id]\n",
    "          decoder_state = beam.decoder_state\n",
    "          y_1_to_t = beam.tokens\n",
    "          \n",
    "          new_beam = Beam(decoder_state, y_1_to_t + [y_t_plus_1], score)\n",
    "          new_beams.append(new_beam)\n",
    "        beams = new_beams\n",
    "        # Set aside completed beams\n",
    "        # move completed beams to `finished` (and remove them from `beams`)\n",
    "        finished += [beam for beam in beams if beam.tokens[-1] == self.eos_id]\n",
    "        new_beams = [beam for beam in beams if beam.tokens[-1] != self.eos_id]\n",
    "        beams = new_beams\n",
    "        \n",
    "        # Break the loop if everything is completed\n",
    "        if len(beams) == 0:\n",
    "            break\n",
    "            \n",
    "    # Return the best hypothesis\n",
    "    if len(finished) > 0:\n",
    "      finished = sorted(finished, key=lambda beam: -beam.score)\n",
    "      return finished[0].tokens, all_attns\n",
    "    else: # when nothing is finished, return an unfinished hypothesis\n",
    "        return beams[0].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4160e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnEncoderDecoder(nn.Module):\n",
    "  def __init__(self, src_field, tgt_field, hidden_size=64, layers=3):\n",
    "    \"\"\"\n",
    "    Initializer. Creates network modules and loss function.\n",
    "    Arguments:\n",
    "        src_field: src field\n",
    "        tgt_field: tgt field\n",
    "        hidden_size: hidden layer size of both encoder and decoder\n",
    "        layers: number of layers of both encoder and decoder\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.src_field = src_field\n",
    "    self.tgt_field = tgt_field\n",
    "    \n",
    "    # Keep the vocabulary sizes available\n",
    "    self.V_src = len(src_field.vocab.itos)\n",
    "    self.V_tgt = len(tgt_field.vocab.itos)\n",
    "    \n",
    "    # Get special word ids\n",
    "    self.padding_id_src = src_field.vocab.stoi[src_field.pad_token]\n",
    "    self.padding_id_tgt = tgt_field.vocab.stoi[tgt_field.pad_token]\n",
    "    self.bos_id = tgt_field.vocab.stoi[tgt_field.init_token]\n",
    "    self.eos_id = tgt_field.vocab.stoi[tgt_field.eos_token]\n",
    "\n",
    "    # Keep hyper-parameters available\n",
    "    self.embedding_size = hidden_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.layers = layers\n",
    "\n",
    "    # Create essential modules\n",
    "    self.word_embeddings_src = nn.Embedding(self.V_src, self.embedding_size)\n",
    "    self.word_embeddings_tgt = nn.Embedding(self.V_tgt, self.embedding_size)\n",
    "\n",
    "    # RNN cells\n",
    "    self.encoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size // 2, # to match decoder hidden size\n",
    "      num_layers    = layers,\n",
    "      bidirectional = True              # bidirectional encoder\n",
    "    )\n",
    "    self.decoder_rnn = nn.LSTM(\n",
    "      input_size    = self.embedding_size,\n",
    "      hidden_size   = hidden_size,\n",
    "      num_layers    = layers,\n",
    "      bidirectional = False             # unidirectional decoder\n",
    "    )\n",
    "\n",
    "    # Final projection layer\n",
    "    self.hidden2output = nn.Linear(2*hidden_size, self.V_tgt) # project the concatenation to logits\n",
    "   \n",
    "    # Create loss function\n",
    "    self.loss_function = nn.CrossEntropyLoss(reduction='sum', \n",
    "                                             ignore_index=self.padding_id_tgt)\n",
    "\n",
    "  def forward_encoder(self, src, src_lengths):\n",
    "    \"\"\"\n",
    "    Encodes source words `src`.\n",
    "    Arguments:\n",
    "        src: src batch of size (max_src_len, bsz)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "    Returns:\n",
    "        memory_bank: a tensor of size (src_len, bsz, hidden_size)\n",
    "        (final_state, context): `final_state` is a tuple (h, c) where h/c is of size \n",
    "                                (layers, bsz, hidden_size), and `context` is `None`. \n",
    "    \"\"\"\n",
    "    src = src.to(device)\n",
    "    src_legnths = src_lengths.to(device)\n",
    "    embeddings = self.word_embeddings_src(src).to(device)\n",
    "\n",
    "    src_lengths = src_lengths.tolist()\n",
    "\n",
    "    packed = pack(embeddings, src_lengths)\n",
    "    o, (h, c) = self.encoder_rnn(packed)\n",
    "    memory_bank, output_lens = unpack(o)\n",
    "    \n",
    "    h = h.reshape(self.layers, 2, -1, self.hidden_size//2)\n",
    "    h = h.transpose(1,2)\n",
    "    h = h.reshape(self.layers, -1, self.hidden_size)\n",
    "    \n",
    "    c = c.reshape(self.layers, 2, -1, self.hidden_size//2)\n",
    "    c = c.transpose(1,2)\n",
    "    c = c.reshape(self.layers, -1, self.hidden_size)\n",
    "    \n",
    "    final_state = (h, c)\n",
    "    context = None\n",
    "    \n",
    "    return memory_bank, (final_state, context)\n",
    "\n",
    "  def forward_decoder(self, encoder_final_state, tgt_in, memory_bank, src_mask):\n",
    "    \"\"\"\n",
    "    Decodes based on encoder final state, memory bank, src_mask, and ground truth \n",
    "    target words.\n",
    "    Arguments:\n",
    "        encoder_final_state: (final_state, None) where final_state is the encoder\n",
    "                             final state used to initialize decoder. None is the\n",
    "                             initial context (there's no previous context at the\n",
    "                             first step).\n",
    "        tgt_in: a tensor of size (tgt_len, bsz)\n",
    "        memory_bank: a tensor of size (src_len, bsz, hidden_size), encoder outputs \n",
    "                     at every position\n",
    "        src_mask: a tensor of size (src_len, bsz): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "    Returns:\n",
    "        Logits of size (tgt_len, bsz, V_tgt) (before the softmax operation)\n",
    "    \"\"\"\n",
    "    max_tgt_length = tgt_in.size(0)\n",
    "    tgt_in = tgt_in.to(device)\n",
    "    memory_bank = memory_bank.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "   \n",
    "    # Initialize decoder state, note that it's a tuple (state, context) here\n",
    "    decoder_states = encoder_final_state\n",
    "    \n",
    "    all_logits = []\n",
    "    for i in range(max_tgt_length):\n",
    "      logits, decoder_states, attn = \\\n",
    "        self.forward_decoder_incrementally(decoder_states, \n",
    "                                           tgt_in[i], \n",
    "                                           memory_bank,\n",
    "                                           src_mask,\n",
    "                                           normalize=False)\n",
    "      all_logits.append(logits)             # list of bsz, vocab_tgt\n",
    "    all_logits = torch.stack(all_logits, 0) # tgt_len, bsz, vocab_tgt\n",
    "    return all_logits\n",
    "\n",
    "  def forward(self, src, src_lengths, tgt_in):\n",
    "    \"\"\"\n",
    "    Performs forward computation, returns logits.\n",
    "    Arguments:\n",
    "        src: src batch of size (max_src_len, bsz)\n",
    "        src_lengths: src lengths of size (bsz)\n",
    "        tgt_in:  a tensor of size (tgt_len, bsz)\n",
    "    \"\"\"\n",
    "    src_mask = src.ne(self.padding_id_src) # max_src_len, bsz\n",
    "    # Forward encoder\n",
    "    memory_bank, encoder_final_state = self.forward_encoder(src, src_lengths)\n",
    "    # Forward decoder\n",
    "    logits = self.forward_decoder(encoder_final_state, tgt_in, memory_bank, src_mask)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "  def forward_decoder_incrementally(self, prev_decoder_states, tgt_in_onestep, \n",
    "                                    memory_bank, src_mask,\n",
    "                                    normalize=True):\n",
    "    \"\"\"\n",
    "    Forward the decoder for a single step with token `tgt_in_onestep`.\n",
    "    This function will be used both in `forward_decoder` and in beam search.\n",
    "    Note that bsz can be greater than 1.\n",
    "    Arguments:\n",
    "        prev_decoder_states: a tuple (prev_decoder_state, prev_context). `prev_context`\n",
    "                             is `None` for the first step\n",
    "        tgt_in_onestep: a tensor of size (bsz), tokens at one step\n",
    "        memory_bank: a tensor of size (src_len, bsz, hidden_size), encoder outputs \n",
    "                     at every position\n",
    "        src_mask: a tensor of size (src_len, bsz): a boolean tensor, `False` where\n",
    "                  src is padding (we disallow decoder to attend to those places).\n",
    "        normalize: use log_softmax to normalize or not. Beam search needs to normalize,\n",
    "                   while `forward_decoder` does not\n",
    "    Returns:\n",
    "        logits: log probabilities for `tgt_in_token` of size (bsz, V_tgt)\n",
    "        decoder_states: (`decoder_state`, `context`) which will be used for the \n",
    "                        next incremental update\n",
    "        attn: normalized attention scores at this step (bsz, src_len)\n",
    "    \"\"\"\n",
    "    tgt_in_onestep = tgt_in_onestep.to(device)\n",
    "    memory_bank = memory_bank.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    prev_decoder_state, prev_context = prev_decoder_states\n",
    "    \n",
    "    \n",
    "    # get decoder output and state\n",
    "    embeddings_tgt = self.word_embeddings_tgt(tgt_in_onestep).to(device)\n",
    "    \n",
    "    if prev_context is not None:\n",
    "        embeddings_tgt = embeddings_tgt + prev_context \n",
    "        \n",
    "    embeddings_tgt = embeddings_tgt.unsqueeze(0)\n",
    "    \n",
    "    decoder_output, decoder_state = self.decoder_rnn(embeddings_tgt, prev_decoder_state)\n",
    "    \n",
    "    # calculate attention and context using attention function\n",
    "    src_mask = src_mask.transpose(0,1).unsqueeze(1)\n",
    "    attn, context = attention(decoder_output, memory_bank, memory_bank, mask=src_mask)\n",
    "    \n",
    "    decoder_output = decoder_output.squeeze(0)\n",
    "    attn = attn.squeeze(1)\n",
    "    context = context.squeeze(0)\n",
    "    \n",
    "    # calculate logits of concatentation of decoder_output and attn projected to vocab size\n",
    "    concatenated = torch.cat([context, decoder_output], dim=1)\n",
    "    \n",
    "    # get logits from forward function projected to vocabulary size\n",
    "    logits = self.hidden2output(concatenated)\n",
    "    decoder_states = (decoder_state, context)\n",
    "    if normalize:\n",
    "      logits = torch.log_softmax(logits, dim=-1)\n",
    "    return logits, decoder_states, attn\n",
    "\n",
    "  def evaluate_ppl(self, iterator):\n",
    "    \"\"\"Returns the model's perplexity on a given dataset `iterator`.\"\"\"\n",
    "    # Switch to eval mode\n",
    "    self.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    for batch in iterator:\n",
    "      # Input and target\n",
    "      src, src_lengths = batch.src\n",
    "      tgt = batch.tgt # max_length_sql, bsz\n",
    "      tgt_in = tgt[:-1] # remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "      tgt_out = tgt[1:] # remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "      # Forward to get logits\n",
    "      logits = self.forward(src, src_lengths, tgt_in)\n",
    "      # Compute cross entropy loss\n",
    "      loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
    "      total_loss += loss.item()\n",
    "      total_words += tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "    return math.exp(total_loss/total_words)\n",
    "\n",
    "  def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Switch the module to training mode\n",
    "    self.train()\n",
    "    # Use Adam to optimize the parameters\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    best_validation_ppl = float('inf')\n",
    "    best_model = None\n",
    "    # Run the optimization for multiple epochs\n",
    "    for epoch in range(epochs): \n",
    "      total_words = 0\n",
    "      total_loss = 0.0\n",
    "      for batch in tqdm(train_iter):\n",
    "        # Zero the parameter gradients\n",
    "        self.zero_grad()\n",
    "        # Input and target\n",
    "        src, src_lengths = batch.src # text: max_src_length, bsz\n",
    "        tgt = batch.tgt # max_tgt_length, bsz\n",
    "        tgt_in = tgt[:-1] # Remove <eos> for decode input (y_0=<bos>, y_1, y_2)\n",
    "        tgt_out = tgt[1:] # Remove <bos> as target        (y_1, y_2, y_3=<eos>)\n",
    "        bsz = tgt.size(1)\n",
    "        # Run forward pass and compute loss along the way.\n",
    "        logits = self.forward(src, src_lengths, tgt_in)\n",
    "        loss = self.loss_function(logits.view(-1, self.V_tgt), tgt_out.view(-1))\n",
    "        # Training stats\n",
    "        num_tgt_words = tgt_out.ne(self.padding_id_tgt).float().sum().item()\n",
    "        total_words += num_tgt_words\n",
    "        total_loss += loss.item()\n",
    "        # Perform backpropagation\n",
    "        loss.div(bsz).backward()\n",
    "        optim.step()\n",
    "\n",
    "      # Evaluate and track improvements on the validation dataset\n",
    "      validation_ppl = self.evaluate_ppl(val_iter)\n",
    "      self.train()\n",
    "      if validation_ppl < best_validation_ppl:\n",
    "        best_validation_ppl = validation_ppl\n",
    "        self.best_model = copy.deepcopy(self.state_dict())\n",
    "      epoch_loss = total_loss / total_words\n",
    "     \n",
    "  def predict(self, tokens, K, max_T):\n",
    "    (src, src_lengths) = self.src_field.process([tokens])\n",
    "    src = src.to(device)\n",
    "    src_lengths = src_lengths.to(device)\n",
    "\n",
    "    beam_searcher = BeamSearcher(model)\n",
    "    \n",
    "    result = beam_searcher.beam_search(src, src_lengths, K, max_T)\n",
    "    sql_query = \"\"\n",
    "    prediction = result[0]\n",
    "    for i in prediction[1:-1]:\n",
    "      string = self.tgt_field.vocab.itos[i]\n",
    "\n",
    "      sql_query += \" \" + string \n",
    "\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 16 # epochs; we recommend starting with a smaller number like 1\n",
    "LEARNING_RATE = 1e-4 # learning rate\n",
    "\n",
    "# Instantiate and train classifier\n",
    "model = AttnEncoderDecoder(SRC, TGT,\n",
    "  hidden_size    = 1024,\n",
    "  layers         = 1,\n",
    ").to(device)\n",
    "\n",
    "model.train_all(train_iter, val_iter, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "model.load_state_dict(model.best_model)\n",
    "\n",
    "# Evaluate model performance, the expected value should be < 1.2\n",
    "print (f'Validation perplexity: {model.evaluate_ppl(val_iter):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
